{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373036da-2c8f-47dd-af32-72b0e45e9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded successfully!\n",
      "  Shape: (6018, 18)\n",
      "  Features: 17\n",
      "  Samples: 6018\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "df = pd.read_csv('cars_cleaned_encoded.csv')\n",
    "print(f\"‚úì Data loaded successfully!\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Features: {df.shape[1] - 1}\")\n",
    "print(f\"  Samples: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af28aff3-a807-4382-a76f-a5269438d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Price']\n",
    "X = df.drop('Price', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "models = {}\n",
    "models['Simple Linear Regression'] = LinearRegression()\n",
    "models['Multiple Linear Regression'] = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "models['Polynomial Regression (Degree 2)'] = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "models['Ridge Regression'] = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "models['Lasso Regression'] = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Lasso(alpha=1.0, random_state=42))\n",
    "])\n",
    "models['ElasticNet Regression'] = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8bf882c-3705-4cd0-b5ab-ebe98689018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Simple Linear Regression... ‚úì Done\n",
      "\n",
      "Training Multiple Linear Regression... ‚úì Done\n",
      "\n",
      "Training Polynomial Regression (Degree 2)... ‚úì Done\n",
      "\n",
      "Training Ridge Regression... ‚úì Done\n",
      "\n",
      "Training Lasso Regression... ‚úì Done\n",
      "\n",
      "Training ElasticNet Regression... ‚úì Done\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\", end=' ')\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for TRAINING set\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    \n",
    "    # Calculate metrics for TEST set\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Calculate accuracy (R¬≤ as percentage)\n",
    "    train_accuracy = train_r2 * 100\n",
    "    test_accuracy = test_r2 * 100\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_predictions': y_train_pred,\n",
    "        'test_predictions': y_test_pred,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "208bb0e6-9d9b-4649-8377-36412af09889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MODEL PERFORMANCE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "                           Model  Train_Accuracy(%)  Test_Accuracy(%)  Train_R2  Test_R2  Test_RMSE  Test_MAE\n",
      "Polynomial Regression (Degree 2)          88.400164         86.631528  0.884002 0.866315   2.076085  1.401068\n",
      "                Ridge Regression          83.563189         82.223786  0.835632 0.822238   2.393999  1.741412\n",
      "        Simple Linear Regression          83.563204         82.220781  0.835632 0.822208   2.394201  1.741557\n",
      "      Multiple Linear Regression          83.563204         82.220781  0.835632 0.822208   2.394201  1.741557\n",
      "           ElasticNet Regression          74.942680         74.281276  0.749427 0.742813   2.879579  2.192910\n",
      "                Lasso Regression          74.861586         73.773118  0.748616 0.737731   2.907887  2.181854\n",
      "\n",
      "======================================================================\n",
      "üèÜ BEST MODEL: Polynomial Regression (Degree 2)\n",
      "   Test Accuracy: 86.63%\n",
      "   Test R¬≤ Score: 0.8663\n",
      "   Test RMSE: 2.08\n",
      "   Test MAE: 1.40\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\" MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train_Accuracy(%)': [results[m]['train_accuracy'] for m in results],\n",
    "    'Test_Accuracy(%)': [results[m]['test_accuracy'] for m in results],\n",
    "    'Train_R2': [results[m]['train_r2'] for m in results],\n",
    "    'Test_R2': [results[m]['test_r2'] for m in results],\n",
    "    'Test_RMSE': [results[m]['test_rmse'] for m in results],\n",
    "    'Test_MAE': [results[m]['test_mae'] for m in results]\n",
    "})\n",
    "\n",
    "# Sort by Test Accuracy (descending)\n",
    "comparison_df = comparison_df.sort_values('Test_Accuracy(%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_test_accuracy = comparison_df.iloc[0]['Test_Accuracy(%)']\n",
    "best_test_r2 = comparison_df.iloc[0]['Test_R2']\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_test_accuracy:.2f}%\")\n",
    "print(f\"   Test R¬≤ Score: {best_test_r2:.4f}\")\n",
    "print(f\"   Test RMSE: {results[best_model_name]['test_rmse']:.2f}\")\n",
    "print(f\"   Test MAE: {results[best_model_name]['test_mae']:.2f}\")\n",
    "print(f\"{'=' * 70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb6d672-f12f-4b7a-9cc2-67b33053f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation (5-Fold) for All Models\n",
      "======================================================================\n",
      "\n",
      "Simple Linear Regression: Mean R¬≤ = 0.8335 (¬±0.0100)\n",
      "\n",
      "Multiple Linear Regression: Mean R¬≤ = 0.8335 (¬±0.0100)\n",
      "\n",
      "Polynomial Regression (Degree 2): Mean R¬≤ = 0.8400 (¬±0.0458)\n",
      "\n",
      "Ridge Regression: Mean R¬≤ = 0.8335 (¬±0.0101)\n",
      "\n",
      "Lasso Regression: Mean R¬≤ = 0.7468 (¬±0.0068)\n",
      "\n",
      "ElasticNet Regression: Mean R¬≤ = 0.7481 (¬±0.0043)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross-Validation (5-Fold) for All Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\", end=' ')\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, \n",
    "                                cv=5, scoring='r2', n_jobs=-1)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    cv_results[name] = {'mean': cv_mean, 'std': cv_std}\n",
    "    print(f\"Mean R¬≤ = {cv_mean:.4f} (¬±{cv_std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369fe5e9-b6fa-4ef8-ad41-ec92f3ac10da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating Visualizations\n",
      "======================================================================\n",
      "‚úì Saved: accuracy_comparison.png\n",
      "‚úì Saved: r2_score_comparison.png\n",
      "‚úì Saved: rmse_comparison.png\n",
      "‚úì Saved: actual_vs_predicted_best.png\n",
      "‚úì Saved: residuals_plot_best.png\n"
     ]
    }
   ],
   "source": [
    "print(\" Creating Visualizations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Plot 1: Accuracy Comparison (Train vs Test)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Train_Accuracy(%)'], width, \n",
    "               label='Train Accuracy', color='lightblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Test_Accuracy(%)'], width, \n",
    "               label='Test Accuracy', color='coral', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Accuracy Comparison (Train vs Test)', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: accuracy_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: R¬≤ Score Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "colors = ['gold' if model == best_model_name else 'skyblue' \n",
    "          for model in comparison_df['Model']]\n",
    "bars = ax.barh(comparison_df['Model'], comparison_df['Test_R2'], \n",
    "               color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax.set_xlabel('R¬≤ Score (Test Set)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Comparison - R¬≤ Score', fontweight='bold', fontsize=14)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "for i, (bar, value) in enumerate(zip(bars, comparison_df['Test_R2'])):\n",
    "    ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{value:.4f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('r2_score_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: r2_score_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 3: RMSE Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "colors = ['gold' if model == best_model_name else 'lightcoral' \n",
    "          for model in comparison_df['Model']]\n",
    "bars = ax.barh(comparison_df['Model'], comparison_df['Test_RMSE'], \n",
    "               color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax.set_xlabel('RMSE (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Comparison - Root Mean Squared Error', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, value) in enumerate(zip(bars, comparison_df['Test_RMSE'])):\n",
    "    ax.text(value + 1000, bar.get_y() + bar.get_height()/2, \n",
    "            f'{value:.0f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rmse_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: rmse_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 4: Actual vs Predicted (Best Model)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "best_predictions = results[best_model_name]['test_predictions']\n",
    "ax.scatter(y_test, best_predictions, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "        'r--', lw=3, label='Perfect Prediction')\n",
    "ax.set_xlabel('Actual Price', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Predicted Price', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Actual vs Predicted - {best_model_name}\\nR¬≤ = {best_test_r2:.4f}', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('actual_vs_predicted_best.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: actual_vs_predicted_best.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 5: Residuals Plot (Best Model)\n",
    "residuals = y_test - best_predictions\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(best_predictions, residuals, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\n",
    "ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Predicted Price', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Residuals', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Residual Plot - {best_model_name}', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_plot_best.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: residuals_plot_best.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c75e30c8-383e-4574-a4fe-9409fb75251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search - Hyperparameter Tuning for Ridge, Lasso & ElasticNet\n",
      "======================================================================\n",
      "\n",
      "A. Grid Search for Ridge Regression...\n",
      "‚úì Best Ridge Alpha: 10\n",
      "‚úì Best CV R¬≤ Score: 0.8335\n",
      "‚úì Test R¬≤ Score: 0.8225\n",
      "\n",
      "B. Grid Search for Lasso Regression...\n",
      "‚úì Best Lasso Alpha: 0.01\n",
      "‚úì Best CV R¬≤ Score: 0.8335\n",
      "‚úì Test R¬≤ Score: 0.8225\n",
      "\n",
      "C. Grid Search for ElasticNet Regression...\n",
      "‚úì Best ElasticNet Alpha: 0.01\n",
      "‚úì Best ElasticNet L1_ratio: 0.9\n",
      "‚úì Best CV R¬≤ Score: 0.8335\n",
      "‚úì Test R¬≤ Score: 0.8225\n",
      "\n",
      "======================================================================\n",
      "Grid Search Results Summary\n",
      "======================================================================\n",
      "\n",
      "             Model  Test_R2  Test_Accuracy(%)\n",
      "     Ridge (Tuned) 0.822469         82.246941\n",
      "     Lasso (Tuned) 0.822492         82.249159\n",
      "ElasticNet (Tuned) 0.822541         82.254060\n"
     ]
    }
   ],
   "source": [
    "print(\"Grid Search - Hyperparameter Tuning for Ridge, Lasso & ElasticNet\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Grid Search for Ridge\n",
    "print(\"\\nA. Grid Search for Ridge Regression...\")\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "ridge_params = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge_pipeline, ridge_params, cv=5, \n",
    "                         scoring='r2', n_jobs=-1, verbose=0)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úì Best Ridge Alpha: {ridge_grid.best_params_['regressor__alpha']}\")\n",
    "print(f\"‚úì Best CV R¬≤ Score: {ridge_grid.best_score_:.4f}\")\n",
    "\n",
    "ridge_test_score = ridge_grid.score(X_test, y_test)\n",
    "print(f\"‚úì Test R¬≤ Score: {ridge_test_score:.4f}\")\n",
    "\n",
    "# Grid Search for Lasso\n",
    "print(\"\\nB. Grid Search for Lasso Regression...\")\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Lasso(random_state=42))\n",
    "])\n",
    "\n",
    "lasso_params = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "lasso_grid = GridSearchCV(lasso_pipeline, lasso_params, cv=5, \n",
    "                         scoring='r2', n_jobs=-1, verbose=0)\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úì Best Lasso Alpha: {lasso_grid.best_params_['regressor__alpha']}\")\n",
    "print(f\"‚úì Best CV R¬≤ Score: {lasso_grid.best_score_:.4f}\")\n",
    "\n",
    "lasso_test_score = lasso_grid.score(X_test, y_test)\n",
    "print(f\"‚úì Test R¬≤ Score: {lasso_test_score:.4f}\")\n",
    "\n",
    "# Grid Search for ElasticNet\n",
    "print(\"\\nC. Grid Search for ElasticNet Regression...\")\n",
    "elastic_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ElasticNet(random_state=42))\n",
    "])\n",
    "\n",
    "elastic_params = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(elastic_pipeline, elastic_params, cv=5, \n",
    "                           scoring='r2', n_jobs=-1, verbose=0)\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úì Best ElasticNet Alpha: {elastic_grid.best_params_['regressor__alpha']}\")\n",
    "print(f\"‚úì Best ElasticNet L1_ratio: {elastic_grid.best_params_['regressor__l1_ratio']}\")\n",
    "print(f\"‚úì Best CV R¬≤ Score: {elastic_grid.best_score_:.4f}\")\n",
    "\n",
    "elastic_test_score = elastic_grid.score(X_test, y_test)\n",
    "print(f\"‚úì Test R¬≤ Score: {elastic_test_score:.4f}\")\n",
    "\n",
    "# Compare Grid Search Results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Grid Search Results Summary\")\n",
    "print(\"=\" * 70)\n",
    "grid_comparison = pd.DataFrame({\n",
    "    'Model': ['Ridge (Tuned)', 'Lasso (Tuned)', 'ElasticNet (Tuned)'],\n",
    "    'Test_R2': [ridge_test_score, lasso_test_score, elastic_test_score],\n",
    "    'Test_Accuracy(%)': [ridge_test_score*100, lasso_test_score*100, elastic_test_score*100]\n",
    "})\n",
    "print(\"\\n\" + grid_comparison.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d33c80-7fc9-451b-8115-36f324102a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Best model saved as: best_model.pkl\n",
      "  Model: Polynomial Regression (Degree 2)\n",
      "‚úì Best model also saved as: best_model.joblib\n",
      "‚úì Feature names saved as: feature_names.pkl\n",
      "‚úì Model info saved as: model_info.pkl\n",
      "\n",
      "======================================================================\n",
      "‚úì ALL FILES SAVED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Files created:\n",
      "  1. best_model.pkl - Your trained model\n",
      "  2. best_model.joblib - Alternative format\n",
      "  3. feature_names.pkl - Column names for prediction\n",
      "  4. model_info.pkl - Model metadata\n"
     ]
    }
   ],
   "source": [
    "#SAVING BEST MODEL AND SCALER\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "# Save the model using pickle\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "print(f\"\\n‚úì Best model saved as: best_model.pkl\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "print(f\"‚úì Best model also saved as: best_model.joblib\")\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "with open('feature_names.pkl', 'wb') as file:\n",
    "    pickle.dump(feature_names, file)\n",
    "print(f\"‚úì Feature names saved as: feature_names.pkl\")\n",
    "\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': best_test_accuracy,\n",
    "    'test_r2': best_test_r2,\n",
    "    'test_rmse': results[best_model_name]['test_rmse'],\n",
    "    'test_mae': results[best_model_name]['test_mae'],\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names)\n",
    "}\n",
    "\n",
    "with open('model_info.pkl', 'wb') as file:\n",
    "    pickle.dump(model_info, file)\n",
    "print(f\"‚úì Model info saved as: model_info.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì ALL FILES SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  1. best_model.pkl - Your trained model\")\n",
    "print(\"  2. best_model.joblib - Alternative format\")\n",
    "print(\"  3. feature_names.pkl - Column names for prediction\")\n",
    "print(\"  4. model_info.pkl - Model metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7a888-bb39-47f4-bbb6-f66b449123cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
